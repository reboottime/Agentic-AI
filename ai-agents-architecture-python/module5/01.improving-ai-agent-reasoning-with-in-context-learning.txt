I want to take you back to that intern analogy again, and I want to think about the different ways that we teach someone to perform a task, or we instruct them to perform a task. And one of the ways we do it is with explicit instructions. We've been doing that over and over. We're telling the agentic AI, here's what you're going to go and do. You're going to do it step by step. But one of the ways that we teach, for example, interns in the real world is we go and show them examples of the problem being solved correctly. So if we want them to write a report, we say, here's the report. Here is my thought process and how I put together this report. Here's the final result. Or we want them to go and diagnose an issue in software, if they're a developer, we say, here was the feature I was trying to implement. Here's how I walked through debugging it or building it or designing it, whatever that thought process was, here's the result. And it turns out we can do the same thing with generative AI through in-context learning. Basically, what this is, is where we do the same thing that we do with an intern, where we teach through examples, but we can do it with generative AI. And in context learning is incredibly powerful. And for many, many agentic AI systems, it is difficult to get it to perform well without examples, without relying on in context learning. It can be really, really tough to get it to perform as well as we want it to. So, I'm going to give you an example of this and the power of this. So, I want to take you back to that alien spaceship escape example I had. So, I had all these tools that had lost contacts because I changed the names and I had removed the descriptions. And now it doesn't know how to problem solve with them because it doesn't know what they do. Now, I could go and explicitly say this is what the tool does, but I'm going to do something different. Instead of telling it what the tool does, I'm going to show it examples of using the tools to solve problems so it can learn what the tools do from seeing how they were used. So here are my tools. Now, I've changed the ordering and the naming slightly, so bear with me. X155, Q63, L199, totally incomprehensible names, no descriptions. Now what I'm doing is I'm going to tap into in context learning, I'm using few shot examples. Basically, I'm giving it examples of solving a problem. So I say, example tool use. Problem: feeling hungry. Thought: I need to prepare food. Tool: q63. Result: alien pizza prepared. Now, you as a human being can look at this, and you can say, okay, the q63 tool has something to do with cooking pizza, or cooking or making pizzas materialize out of thin air. Next example. Problem, need to move to other world. Thought, I need a wormhole. Tool, X155. Result, wormhole open to Vanderbilt University. And then I go on to the next one. Problem, I need to get to the anti gravity room. Thought, I need a method of transportation around the ship. Tool L 199 result, I'm riding on a scooter towards the room, and so I haven't explicitly told it what the tools do, but I've told it if I have this problem, this is my thought process behind solving it. Here is the tool I'm going to use, and then I'm showing it the result of that tool's application. And so I'm teaching it how to solve problems and to think about solving problems, and I'm showing it in that context how the tools are actually doing things, but I'm not telling it. This tool does this very different. This is like what we would do with a human being. So what do we get? Problem, I need to get off the ship to Earth! Thought colon. Now, one of the things that, in context, learning does is it establishes a pattern that the large language model wants to follow. And so if I show it an incomplete part of a pattern, it wants to finish the pattern. So these models were taught to, you know, roughly predict the next word in a sentence at a very high level, we can think about it that way. So, if I show it an incomplete sentence, it wants to complete it. If I show it a partial pattern of solving a problem, it wants to finish solving the problem with one of the tools, and it wants to solve it in a way that is similar to what was done before, because it wants to stick its reasoning into that pattern, its output should fall into that pattern. So when I pick up in the middle of the pattern and I say, problem, I need to get off the ship to earth, and just leave it with thought, it now says, problem, I need to get off the ship to Earthen. Thought, I need a way to locate the nearest escape pod. Tool X155. Result, wormhole open to Vanderbilt University. So now I can escape the ship. And if I then introduce new things, what's interesting, because remember, I didn't tell it, I told it tools that were available to it. And I told it, pattern on how to solve. And now, what happens when I, for example, give it a new tool at its disposal? So. I say, result, warm hole open to Vanderbilt University. New tool added, inner wormhole. Now it says, problem, I need to get back to Earth from Vanderbilt University, which is kind of funny, because now it thinks that Vanderbilt is not on earth. Thought, I need to use the wormhole tool to travel back to Earth. Tool, inner wormhole. And so, it may not be completely accurate in his description of Vanderbilt University not being on Earth, but it is recognizing now that this new tool that was introduced, it can use to escape by going into the wormhole. But the more important thing is, it followed the pattern. It again restated what the current problem is. It explained its thought behind it, and then explained the tool that it was going to use. So it picked up on the pattern. In many cases, this is going to be really important when we think about interfacing with our real computer systems, where the problem solving process, the steps that we use to solve the problem, must follow well defined patterns, because these systems can't deal with arbitrary patterns, they can't deal with arbitrary language. So using in context learning helps us not only to teach it about things like the tools, but it also helps us to teach it patterns of the steps that we follow, patterns behind the reasoning, like the thinking. And that's can, when we say thought or think step by step, all these things like that around, we're showing it thought processes, decomposition of the problem into thoughts, you know, based on the thought, what tool to use, these types of things, we often are also teaching it not only how to problem solve, but how to problem solve in a pattern or sequence that will work with the systems that we need it to work with. Very, very powerful concept, very, very important tool to use. If you're working with an agentic AI system and you're having trouble with the prompts behind it, and you want to figure out, what do I do to get it more constrained, more sort of, like, unified and predictable in its behavior? Start by looking at in context learning and giving it examples of what you want it to do, and often that can help but constrain it. And if you think about it, if you went to an intern and you tried to explain to an intern how to write like Tolstoy, that's going to be a very long list of rules. But you could probably get that intern up to speed much faster by simply showing them a few pages of Tolsoi's work. And so many problems it's going to be easier to describe it in examples than it is going to be to describe it in rules or instructions that, may not cover every edge case and may not be perfectly crisp and understandable. Let's look at another example of this. I go back to the microwave example, and in this case, what I want it to do is I want it to learn the concept of time that I want it to infer that increased time is going to be by 5 seconds. And therefore it needs to use that microwave increase time tool over and over in order to get the time to what it wants. Now, before it told me, hey, just go and increase the time until you get to 60 seconds. So I show it the tools again, similar tools to last time with the microwave. Now I show it example tool usage. And I say, problem, reheat leftover pizza. Thought, I need to heat the pizza for 20 seconds. And then I show it applying. First, I say, open microwave door. Closed door. And then I show it increasing the time four times, which is four times five is 20 seconds. So I'm showing it how to do that and then the result. And then I do another one. I'm going to soften ice cream, and this one is going to be 10 seconds. So it's two applications now. I say, I'm warming up food in the microwave. You will output one step at a time with a problem thought and one or more tool steps. The system will provide the result after you output all your tool steps. I need to melt some shredded cheddar on nachos. So, I've put in that problem thought tool format again, but this time I'm expecting multiple tool outputs. So, I need to melt some shredded cheddar on nachos thought. And it comes back and it says, tool, microwave, open door. Tool, microwave closed door. It's picking up on the pattern that, okay, I need to open the door, put the food in, close the door, and then it says 30 seconds. And it translates that into six. Microwave, increase time tool calls. So now what we've moved is even closer to being able to interface with a real computing system. Does that set of tool calls and then says, please provide the result. I say, heated food or food heated, sitting in microwave. Now it problem solves a little bit. Open door. I tell it, result, food heated sitting in microwave door open. It queries again and it says, okay, now you've, it's gone and gotten the food out, so close the door. So, I've now got the nachos ready to serve. So, in context learning is critically important many times for agentic AI. Particularly, we want it to interact with computer systems that are brittle, fragile, very, they have very limited formats. They are not like a human being. They do not want to accept weird things that they have never seen before. A human will gladly interpret and figure it out. Computer systems do not do this. And the moment that we start needing that rigid pattern, we typically also often need in context learning to start showing it the patterns that are acceptable to those computer systems and start teaching it how to interact with them in structures that are acceptable. How to query a database, how to go and interact with an exchange server to send an email, how to do x. We go and we give it examples of sort of fake examples of using that thing correctly in order to solve the problem. Or we get blog some way we capture real traces of solving that problem in that language and we show it to it and say, this is how you do it. Critically important to use in context learning. When you're going and trying to build these agents up and you're trying to think, well, how do I get it to work? Why is it not working? It keeps failing. It keeps trying to interact with the computer system, and computer keeps like, having absolute fit over what's being set. Think about going and adding a few shot examples or even tons of examples to get it to work.
