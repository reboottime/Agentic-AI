# Advanced Agent Interaction

## Selective Memory Sharing: Using LLM Understanding for Context Selection

Sometimes we want an agent to intelligently choose which parts of its memory to share with another agent. Instead of using rigid rules, we can leverage the LLM's understanding of context to select the most relevant memories for the task at hand.

Let's implement a version of memory sharing that uses the LLM to analyze and select relevant memories with self-prompting:

```python
@register_tool(description="Delegate a task to another agent with selected context")
def call_agent_with_selected_context(action_context: ActionContext,
                                     agent_name: str,
                                     task: str) -> dict:
    """Call agent with LLM-selected relevant memories."""
    agent_registry = action_context.get_agent_registry()
    agent_run = agent_registry.get_agent(agent_name)
    
    # Get current memory and add IDs
    current_memory = action_context.get_memory()
    memory_with_ids = []
    for idx, item in enumerate(current_memory.items):
        memory_with_ids.append({
            **item,
            "memory_id": f"mem_{idx}"
        })
    
    # Create schema for memory selection
    selection_schema = {
        "type": "object",
        "properties": {
            "selected_memories": {
                "type": "array",
                "items": {
                    "type": "string",
                    "description": "ID of a memory to include"
                }
            },
            "reasoning": {
                "type": "string",
                "description": "Explanation of why these memories were selected"
            }
        },
        "required": ["selected_memories", "reasoning"]
    }
    
    # Format memories for LLM review
    memory_text = "\n".join([
        f"Memory {m['memory_id']}: {m['content']}"
        for m in memory_with_ids
    ])
    
    # Ask LLM to select relevant memories
    selection_prompt = f"""Review these memories and select the ones relevant for this task:

Task: {task}

Available Memories:
{memory_text}

Select memories that provide important context or information for this specific task.
Explain your selection process."""

    # Self-prompting magic to find the most relevant memories
    selection = prompt_llm_for_json(
        action_context=action_context,
        schema=selection_schema,
        prompt=selection_prompt
    )
    
    # Create filtered memory from selection
    filtered_memory = Memory()
    selected_ids = set(selection["selected_memories"])
    for item in memory_with_ids:
        if item["memory_id"] in selected_ids:
            # Remove the temporary memory_id before adding
            item_copy = item.copy()
            del item_copy["memory_id"]
            filtered_memory.add_memory(item_copy)
    
    # Run the agent with selected memories
    result_memory = agent_run(
        user_input=task,
        memory=filtered_memory
    )
    
    # Add results and selection reasoning to original memory
    current_memory.add_memory({
        "type": "system",
        "content": f"Memory selection reasoning: {selection['reasoning']}"
    })
    
    for memory_item in result_memory.items:
        current_memory.add_memory(memory_item)
    
    return {
        "result": result_memory.items[-1].get("content", "No result"),
        "shared_memories": len(filtered_memory.items),
        "selection_reasoning": selection["reasoning"]
    }
```

This implementation makes memory selection more intelligent and transparent:

1. Each memory gets assigned a unique ID for reference.

2. The complete set of memories is presented to the LLM with their IDs.

3. The LLM analyzes the memories in the context of the specific task and selects the relevant ones using structured JSON output.

4. The LLM provides reasoning for its selection, which is preserved in the original agent's memory.

For example, if a project management agent is delegating a budget review task, the interaction might look like this:

```python
# Example memory contents:
memories = [
    {"type": "user", "content": "We need to build a new reporting dashboard"},
    {"type": "assistant", "content": "Initial cost estimate: $50,000"},
    {"type": "user", "content": "That seems high"},
    {"type": "assistant", "content": "Breakdown: $20k development, $15k design..."},
    {"type": "system", "content": "Project deadline updated to Q3"},
    {"type": "user", "content": "Can we reduce the cost?"}
]

# LLM's selection might return:
{
    "selected_memories": ["mem_1", "mem_3", "mem_5"],
    "reasoning": "Selected memories containing cost information and the request for cost reduction, excluding project timeline and general discussion as they're not directly relevant to the budget review task."
}
```

The second agent then receives only the memories about cost estimates, breakdowns, and the request for reduction, giving it focused context for its budget review task without extraneous information about timelines or other project aspects.

This approach has several advantages over rule-based filtering:

1. The selection process can understand context and implications, not just match patterns.

2. The reasoning is preserved, helping track why certain information was or wasn't shared.

3. The selection can adapt to different types of tasks and contexts without changing the code.

4. The original agent maintains a record of what information was shared and why.

This pattern is valuable when you want to provide specific context without overwhelming the second agent with irrelevant information. For example, if a project planning agent asks a budget specialist to review costs, it might share only the memories related to resource allocation and expenses, not the entire project history.

## Recap of the Four Memory Sharing Patterns

Each of these patterns serves a different purpose in agent collaboration:

- Message passing keeps interactions simple and focused
- Memory reflection helps agents learn from each other's processes
- Memory handoff enables seamless continuation of complex tasks
- Selective memory sharing provides relevant context while reducing noise

The choice of pattern depends on your specific needs:

- How much context does the second agent need?
- Does the first agent need to understand the second agent's process?
- Should the conversation history be preserved?
- Is there sensitive information that should be filtered?

By understanding these patterns, you can design agent interactions that effectively balance information sharing with task focus, leading to more efficient and capable multi-agent systems.