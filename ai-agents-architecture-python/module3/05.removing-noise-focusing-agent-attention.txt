Now there's one issue that we haven't talked about yet when we think about sharing memory between the agents. If we go back and we think about this task model where we have one agent send a message to another agent and so this is our task and that's our message that we're sending, there's one really important sort of bit of detail that we left out and that is in this model there's always going to be some limit to the size of this message because inherently in order for this agent to go and create that task it's going to have to generate a bunch of tokens. It's basically have to generate a bunch of output for that message so if you think of it as a tool call it's gonna have to fill in that parameter that describes that task in order to give it to the other agent and every single LLM has some limit on the number of tokens that we can output. So we have a limit on the number of tokens we can take in and we have a limit on the numbers of tokens we can take out. With most LLMs we typically can get tokens into the LLM, we can get more tokens into the LLM as input than we can get out and it's more expensive to get output tokens than it is to get input tokens. So what we see is we often have asymmetries where we have a lot of tokens going in so we'll have a whole bunch of tokens going into the LLM and then we'll have a relatively smaller trickle of tokens going out so we'll have this huge volume going in and then a smaller volume going out on the other side. So what that means is is when we go and we're passing messages is that we can have a fixed size message. There's going to be a limit to how big the message is. Now why does this matter? Well if we think about the memory, the memory is this side. This is our memory over here. This is the output size which is the task and so if we think of our models that we've looked at the memory can have much more information and can be much bigger in size than our task description. So whenever we do this message passing model we're gonna have to work with the fact that our task is inherently going to have to be much smaller and sometimes what we're gonna see is it's really hard to get the LLM to collect all the information we need and to put it into the task. At the same time just handing off the entire memory can be expensive, it can be slow, it can lead to big prompts, it also may expose the other agent to things it doesn't need to know about. So there is a middle ground and it's an interesting middle ground and it's essentially a employing an interesting trick. So let's say this is our memory right and each in our memory is basically a set of messages. We have one, two, three, four, five, etc and we have all these messages in our memory. Now the memory can be very large. What we would like to do is we would like to have an agent that intelligently selects which parts of the memory should be handed off to the other agent and we'd like to do this very very efficiently. We'd then like to select those parts of the memory and incorporate them into the task before we send that task down to the other agent that we're calling to. So we'd like to be able to go and look back at the memory and say these are all the important you know facts and information that this other agent needs we'd like to hand it off. Now the challenge we have is even when we go and do this right there's going to be some limit to this task size. So how do we get as much information into the task as possible when it might be you know some huge memory that we're dealing with that's much bigger than we can output. Well what we can do is we can employ a trick. We can teach our agent to go and look back at all the messages and produce a list of the messages that are relevant to the other agent. So we can produce a list like 1, 3, 4. And so basically what we do is we output the identifiers of the messages that should be sent to the other agent. We're basically allowing one agent to look into the memory, identify which messages are relevant, output a list of the identifiers not the contents of those memories. And this is really important is these IDs are short and efficient. If we have for example numerical IDs that are just the indexes of the messages in the memory it can be really efficient for the LLM to look at those indexes and say well these are the the IDs or the indexes of all the memories that are relevant and output that into a task. And then when we go and get those IDs we can then inflate those into the actual memory. So when we put them into the task we basically go and inflate and go and pull the actual memories that the that the LLM selected as its output and put those into the task description. So what we're having is basically the ability of the LLM to look back to the conversation say these are the IDs of the messages that are relevant. As opposed to going and saying here's everything that's relevant and trying to output the text which could generate very very long output and maybe longer than it can actually output. We simply say go and tell us the IDs of the messages that are relevant. It then goes and outputs those IDs and then we have just a programmatic process and code where we go and inflate those IDs. Basically we go and look up the actual contents of the memory. We put that value in as the string contents. We replace that identifier with the actual contents of the memory. We do that for each of the IDs and then the the memory gets inflated or basically the task description gets inflated and then we hand that off. So it allows us to be able to overcome this token limit, select relevant messages to go in, overcome the token limit that we can get on output, but also do it very efficiently. This is something that looks like a tool call. Outputting a list of memories saying call this agent this other agent and include these memories. Here's a list of the IDs of the memories to include. That is something that is very focused. It is very short output. It gets inflated into the larger context that the other agent needs. It's also very important because the memories themselves, if it's just outputting IDs and selecting IDs, it can't corrupt the value of the memories themselves. It can't go and decide oh I'm going to summarize the results of that operation. All it can do is refer to the results of the operation and pull in the actual contents. So this also helps us from a hallucination standpoint and that when it's pulling in content it can't alter the contents of the memories. It can only reference what already exists and then have that pulled in. So that protects us in another way and that can be important. For example, if we're going and querying an API, we're getting back some results and we really don't want it to change the facts in those results. We just say well you can choose which facts I'm going to pass along and so we can defend ourselves to some degree from that hallucination. We also get the token efficiency that the output now is very small. It's a list of identifiers as opposed to the actual content of all the memories. So that's a huge thing and it allows us to overcome this asymmetry problem where the second agent can take in a lot more information than the first agent can output in a task description. So we get over that by having the first one output identifiers of memories that then get inflated and can fill up that full context window of that second agent. The final thing that this does is it allows, if we design it right, we can actually have an expert essentially select which memories are relevant to the other one. So we can even have an agent or an expert prompt that's sole job is to go and select memories that are relevant to the task that the second agent is going to perform. And so we can alleviate that first agent of that burden. We just make this into a tool. It's basically a selection prompt. Select which of these messages are relevant to this task and then hand off to the other agents. We get all kinds of interesting benefits from this.