I wanna talk about some principles when we're thinking about designing our agents. Now, there's many different principles, like are they friendly, all kinds of things like that, but I want you to think about some computational sort of software design principles, but in the context of agents, that will help you to generate agents or create agents that are more effective, that are more resistant to doing the wrong thing, that cost less, that are faster, more predictable, give us all those sort of principles and qualities we want. Now, there's so many different qualities we want, but I think there's a couple of things that we can boil down to, and if we pay attention to these things, in general, we're gonna end up building better agents. So the first one I wanna talk about is the M, or the model efficiency. Basically, we want to use the most efficient model for the task, right? Now, we can always go and just buy the most expensive, most complex model, and have it run the entire agent loop, but often that's not what we really want, because that's also the most expensive thing. The most capable models are often the slowest models. There's many reasons why we may not wanna run the most expensive, capable model all the time. So model efficiency. We wanna use the right model at the right time. We wanna use the sort of least capable model that will get the job done, because that's simply gonna be the cheapest model and the fastest model. The second thing that we wanna think about is the A, the actions that we get, the actions and the specificity of those actions. Now, when we use more specific actions, typically they're easier to reason about. For example, if you have actions that are very, very concrete, that, for example, create calendar invite, like that's a very concrete action, and it's clear how to use it. If you have a more generic action, like perform calendar operation, they can take in all these different parameters and varying combinations, and many of those combinations of parameters are incorrect. Well, that's a much harder thing to reason about, and you're gonna end up having to use a more complex model. You're probably also going to have more expense, because you're gonna have to have more complex instructions, which leads to many more tokens. So action specificity is something we always wanna think about. We wanna try to get as specific actions as possible, while still having something that's reusable. The other thing that action specificity can get us is we can design actions that are near impossible to use wrong. If you use the action and you're able to call it, like you're more than likely gonna get it right. And it does one simple thing, but it does it very effective, and it does it repeatably and correctly. And so we're going to have specific actions that help our agent and minimize the amount of reasoning it has to do. The other thing that we can do is rather than have some task that we need to perform broken up into like a hundred different individual actions that the model needs to go and figure out how to chain together, it may be very reusable, but from the model's perspective, it's hard to use, because it's not specific to the task. We have lots of individual generic actions. If we group them together into one big action, like schedule a meeting with Bob, the AI can look at that and it's like, oh, I need to schedule a meeting with Bob. What should I do? I should call the schedule meeting with Bob tool. And if there's a complex workflow for scheduling a meeting with Bob, we can build that in code. We can make it repeatable. We can make it so it's easy and correct. And we don't have it going and trying to piece it together from all these weird rules. So action specificity can be really important because it can make it so that we can use a less capable model with fewer tokens often and get things done faster. This is particularly true when we build actions that take a bunch of steps and put them together in code in a repeatable way, which is also typically going to be faster than if we break this thing up into a whole bunch of generic little things that now the LLM has to string together and reason between each individual step. Let's go ahead and have more specific actions. T, token efficiency. We want to think about how efficient we are being with tokens. Why do we want to think about this? One is tokens cost us money, right? Every token that we send into that model, typically we're going to pay for in some form. Either we're running the model ourself on our own hardware, meaning we're running GPUs and resources, and we've got a limited capacity to process tokens. We've got some number of tokens per minute we can process. And so we don't want to waste those tokens. If we're paying for an API for a model, we don't want to waste a bunch of tokens. So token efficiency is something we're going for. Another reason token efficiency is good is not only does it save us money, but often it's faster. Smaller prompts, less output is going to be faster. The other thing is, is that tokens typically can, when we're having fewer tokens, if we have really information-dense tokens, right? If we've got the right information going into the model, we're being very token efficient in how we represent that information and when we can give it to the model, often we can get it to reason better, or we can end up with a less capable model because we're getting the right information. If we have all of this irrelevant information that the model has to see through in order to do the right thing and not have some weird ramifications, so token efficiency is really important. We want those tokens to be information-dense, have the right information for the task so that the agent can get its work done, but also so that we can spend less money and get things done faster. And then the final one is we want the environment, and I'm going to abbreviate this, to be safe. That is, we want to design this in a way that when it takes action in the environment, that it's safe. So, for example, if the agent goes and takes an action, typically we'll want actions that are read, so read actions are pretty darn safe, typically, because if we go and read something, we're not affecting the system. The moment we go and write to the system, now we have a much more dangerous action, actions that have side effects in the environment. You know, an environment where all the actions have weird, unexpected side effects, that's not a very safe environment. So we want to design our environment in a way that when actions are taken, they're safe and typically reversible. So we want to have an environment where we design things so that if you create a calendar invite, you can undo the calendar invite very easily, or we can undo all the work that the agent did. And so we want to think about building safe environments. Now, if you're seeing this, this is mate, basically. This is what we are going for, and if you think about this from chess, we're trying to play a game. We want our agent to play a game, and we want it to win. In chess, you've got to arrive at checkmate, you've got to mate the other person's king in order to win the game, and we're looking at that. So we're trying to think about how do we have really efficient models and use of models, we want to have actions that are specific to the task, we want token efficiency, and we want a safe environment for the agent to execute in, and you can think of this as the MATE acronym.