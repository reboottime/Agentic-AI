When we're building agents, one of the hardest parts is getting the design of the agent, the instructions for the agent, the set of tools that it uses, the format of the information that comes back to it. That is really the hard part. The actual implementation in code is much, much less difficult than actually thinking through what are the instructions, the prompts, the tools, how do we design all of this so it's robust and resilient? Now, one of the best things that we can do is we can try to rapidly iterate. One of the things that we can do to iterate faster is by simulating agents through a conversation. Because at the end of the day, the agent loop is essentially a big automated conversation. We can do exactly what's going on in the agent loop. Well, not totally exactly, but really close simply by going into something like ChatGPT and having a conversation if we set it up in the right way, so it looks very similar to the prompts that the actual agent will be receiving in our agent loop. Now, this is a way of doing rapid prototyping, and let's take an example and go through it and see how we go about doing this. What I'm going to do is I'm going to set up a simulation, and this is the prompt that I'm going to use to build that simulation. I'd like to simulate an AI agent that I'm designing. The agent will be built using the game framework with these components. Goals, document all the code of the project. Actions, list files, and I have fully qualified path, read file, write file. At each step, your output must be an action to take, stop and wait, and I will type in the result of the action as my next message. Ask me for the first task to perform. The goals are specified here, the actions are here, the environment is me. I am the execution environment. When it's going to happen, is this going to output an action? I'm going to go and tell it, here's what the result of your action is. I am becoming the manual human execution environment, which allows me to dream up any tool under the sun and how it might behave, inject errors arbitrarily. Then finally, the memory is essentially just the conversation. I input this prompt and it says, great, what's the first action you'd like to perform? I come back and I say, document the project. Then it goes and basically is running the first iteration of the loop. Our agent loop has been kicked off and it says, list files, fully qualified path. Now, there's many different things I could do at this point. I could allow it to put something like fully qualified path in, or I could be more strict and I can say, you know what? That's not really the behavior I want because I wanted to fill in the path and it didn't. I go back and I say, well, error, no path provided. It just filled in some random variable name and it says, please provide the fully qualified path of the project directory. What this has shown me is, I've got a flaw in my design of my agent. Now, if we go back and we look at the agent design, it had a tool to list the files, read a file, and write a file, but they all defend on a fully qualified path, and we haven't given it a starting path. Now, this could be something we could inject into the memory as a fully qualified starting path. That would be one way of fixing this design flaw. Another way would be to give it some tool to actually go and look at the underlying directory that we're running in, and so it can query that. I'm going to go and do that. I'm going to update the prompt. Basically, all I've done is I've taken my original prompt, I've just adapted it, and I've given it a new tool. Now, notice this tool is really fast to implement because all I have to do is update my prompt and add it as a new action that this thing can take. I dream up why I want it to look like. I'm prototyping in a conversation. I've added a new tool, which is GetProjectDirectory. It goes through, says what task, I say document the project, and now what it does is it runs the GetProjectDirectory task, and then I simulate the output of that tool. I haven't even built it. There's no code. I'm just simulating in a conversation, and then that's all it needs. It can go and then say, okay, list files, and now it gives me the actual fully qualified path of the directory based on the output of my prior tool that I simulated. I'm going to simulate that these are the files that are in the directory. It then comes back and it says, okay, read file. Our agent loop is running, it's running actions each time when it runs an action, we are acting as the environment, we are simulating the output back, and our conversation is the memory that it has. When I say, okay, well, print hello world is all that's in that file. It then comes back, it runs the next action, which is write file. Now, things get interesting, because I've discovered a potential design issue that I might want to fix. I didn't really specify very clearly what it meant to document the project, and it's deciding to overwrite the existing file and add comments to it. So what I've got is an ambiguity in my goals. Now I could go back and I could refactor my goals to make it more explicit about what it's supposed to do, or I could change my tools and my tool design to make it really clear through my tool design, or to prevent this type of thing, for example, I could throw an error if you try to write over an existing file, or I could go and make it so it's create doc, and it passes in the name of the file it's documenting for, and I could take that decision making out of the agent's hands. But you see, this conversation is allowing me to discover those issues and rapidly iterate and change the design of the agent much faster than we can do if we're gonna do this in code and execute and compile or whatever we're dealing with. And then I can go back and continue simulating, see if it handles different languages okay. So now I've got a Java file that's being read, and it goes through and it documents it. I could go and test and see how long can the conversation get, like how many files can I put in that original listing, and give it text back before it breaks down. It becomes much easier to do that initial rapid experimentation. Now I'm still gonna go and do more experimentation after I implement it, but before I even get to implementing it, I wanna go and do this rapid, really cheap experimentation to make sure that I have designed my agent appropriately, that I've got the right goals and instructions in place, I've got the right actions in place, I've got the right format for the information that's coming back from my environment, in this case me, my simulated environment. And I wanna look at how is a conversation gonna work, how long can it get, will it break down, will it be able to get what it needs out of the conversation. And so it gives me that ability to go and control what the agent sees, to inject errors, to identify potential ambiguities, and to make better designs, and improve the decision making of my agent by simulating a conversation, giving it all kinds of interesting situations to handle, to see how it reasons. And then to quickly update the agent as quickly as editing in a prompt.